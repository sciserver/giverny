{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accessing turbulence isotropic cube files on filedb cluster from sciserver\n",
    "* created volume container (turb) showing all /turb folders on filedb system\n",
    "* access to turbinfo for metadata about these files\n",
    " * requires copying DataPath table\n",
    "* morton curve code pip installed in sciserver container\n",
    "* some special code to take into account 8x8x8 blobs with z-y-x ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import math\n",
    "import time\n",
    "#import psutil\n",
    "#import tracemalloc\n",
    "#import struct\n",
    "import numpy as np\n",
    "import SciServer.CasJobs as cj\n",
    "# installs morton-py if necessary.\n",
    "try:\n",
    "    import morton\n",
    "except:\n",
    "    %pip install morton-py\n",
    "    import morton\n",
    "\n",
    "class iso_cube:\n",
    "    def __init__(self, cube_num, cube_dimensions = 3, cube_title = ''):\n",
    "        # cube size.\n",
    "        self.N = cube_num\n",
    "        \n",
    "        # turbulence dataset name, e.g. \"isotropic8192\" or \"isotropic1024fine\".\n",
    "        self.cube_title = cube_title\n",
    "        \n",
    "        # setting up Morton curve.\n",
    "        bits = int(math.log(self.N, 2))\n",
    "        self.mortoncurve = morton.Morton(dimensions = cube_dimensions, bits = bits)\n",
    "        \n",
    "        self.init_cache()\n",
    "        \n",
    "    def init_cache(self):\n",
    "        # read SQL metadata for all of the turbulence data files into the cache\n",
    "        sql = f\"\"\"\n",
    "        select dbm.ProductionMachineName\n",
    "        , dbm.ProductionDatabaseName\n",
    "        , dbm.minLim, dbm.maxLim\n",
    "        , dbm.minTime, dbm.maxTime\n",
    "        , dp.path\n",
    "        from databasemap dbm\n",
    "           join datapath{str(self.N)} dp\n",
    "             on dp.datasetid=dbm.datasetid\n",
    "           and dp.productionmachinename=dbm.productionmachinename\n",
    "           and dp.ProductionDatabaseName=dbm.ProductionDatabaseName\n",
    "        where dbm.datasetname = '{self.cube_title}'\n",
    "        order by minlim\n",
    "        \"\"\"\n",
    "        df = cj.executeQuery(sql, \"turbinfo\")\n",
    "        \n",
    "        x, y, z = self.mortoncurve.unpack(df['minLim'].values)\n",
    "        df['x_min'] = x\n",
    "        df['y_min'] = y\n",
    "        df['z_min'] = z\n",
    "        \n",
    "        x, y, z = self.mortoncurve.unpack(df['maxLim'].values)\n",
    "        df['x_max'] = x\n",
    "        df['y_max'] = y \n",
    "        df['z_max'] = z\n",
    "        \n",
    "        self.cache = df\n",
    "    \n",
    "    # defines some helper functions, all hardcoded (double-check this when other datasets are available)\n",
    "    def parse_corner_points(self, x_min, x_max, y_min, y_max, z_min, z_max):\n",
    "        # only points 1, 2, 4, and 5 are required for finding the correct sub-boxes.\n",
    "        # corner 1 is the bottom left back side origin point.\n",
    "        # corner 2 is the bottom right back side corner point (same as corner 1 except at the maximum x-position).\n",
    "        # corner 4 is the bottom left front side corner point (same as corner 1 except at the maximum y-positon).\n",
    "        # corner 5 is the top left back corner point (same as corner 1 except at the maximum z-positon).\n",
    "        # corners 2, 3, and 4 travel around the bottom plane of the box clockwise from corner 1.\n",
    "        # corners 6, 7, and 8 travel around the top plane of the box clockwise from corner 5.\n",
    "        c1 = (x_min, y_min, z_min)\n",
    "        c2 = (x_max, y_min, z_min)\n",
    "        #c3 = (x_max, y_max, z_min)\n",
    "        c4 = (x_min, y_max, z_min)\n",
    "        c5 = (x_min, y_min, z_max)\n",
    "        #c6 = (x_max, y_min, z_max)\n",
    "        #c7 = (x_max, y_max, z_max)\n",
    "        #c8 = (x_min, y_max, z_max)\n",
    "        \n",
    "        corner_points = (c1, c2, c4, c5)\n",
    "        \n",
    "        return corner_points\n",
    "        \n",
    "    def get_files_for_corner_points(self, x_range, y_range, z_range, var, timepoint):\n",
    "        # define the corner points.\n",
    "        x_min = x_range[0]; x_max = x_range[1];\n",
    "        y_min = y_range[0]; y_max = y_range[1];\n",
    "        z_min = z_range[0]; z_max = z_range[1];\n",
    "        \n",
    "        # retrieve the corner points.\n",
    "        c_points = self.parse_corner_points(x_min, x_max, y_min, y_max, z_min, z_max)\n",
    "        \n",
    "        database_files = []\n",
    "        \n",
    "        # only points 1, 2, 4, and 5 are required for finding the correct sub-boxes.\n",
    "        c1_info = self.get_file_for_point(c_points[0][0], c_points[0][1], c_points[0][2], var, timepoint)\n",
    "        c1_file = c1_info[0]\n",
    "        database_files.append(c1_file)\n",
    "        \n",
    "        c2_info = self.get_file_for_point(c_points[1][0], c_points[1][1], c_points[1][2], var, timepoint)\n",
    "        c2_file = c2_info[0]\n",
    "        database_files.append(c2_file)\n",
    "        \n",
    "        c4_info = self.get_file_for_point(c_points[2][0], c_points[2][1], c_points[2][2], var, timepoint)\n",
    "        c4_file = c4_info[0]\n",
    "        database_files.append(c4_file)\n",
    "        \n",
    "        c5_info = self.get_file_for_point(c_points[3][0], c_points[3][1], c_points[3][2], var, timepoint)\n",
    "        c5_file = c5_info[0]\n",
    "        database_files.append(c5_file)\n",
    "        \n",
    "        return database_files\n",
    "    \n",
    "    def find_sub_box_end_point(self, axis_range, datapoint, axis_position, db_file_comparison, var, timepoint):\n",
    "        # placeholder end point value. \n",
    "        end_point = -1\n",
    "        # if the difference between the axis range end points is <= to this value, then the end_point\n",
    "        # has been found.\n",
    "        axis_range_difference = 2\n",
    "        \n",
    "        end_point_found = False\n",
    "        while not end_point_found:\n",
    "            mid_point = math.floor((axis_range[0] + axis_range[1]) / 2)\n",
    "            \n",
    "            # stops recursively shrinking the box once the difference between the two end points is <= axis_range_difference.\n",
    "            if (axis_range[1] - axis_range[0]) <= axis_range_difference:\n",
    "                end_point_found = True\n",
    "            \n",
    "            # updates the datapoint to the new mid point.\n",
    "            datapoint[axis_position] = mid_point\n",
    "            \n",
    "            # gets the db file for the new datapoint.\n",
    "            datapoint_info = self.get_file_for_point(datapoint[0], datapoint[1], datapoint[2], var, timepoint)\n",
    "            datapoint_file = datapoint_info[0]\n",
    "            \n",
    "            # compares the db file for datapoint to the origin point.\n",
    "            if datapoint_file == db_file_comparison:\n",
    "                end_point = mid_point\n",
    "                axis_range[0] = mid_point\n",
    "            else:\n",
    "                end_point = mid_point - 1\n",
    "                axis_range[1] = mid_point\n",
    "            \n",
    "            # used for checking that there were no redundant calculations\n",
    "            #print(f'midpoint = {mid_point}')\n",
    "            #print(f'endpoint = {end_point}')\n",
    "            #print('-')\n",
    "                \n",
    "        return end_point\n",
    "    \n",
    "    def recursive_single_database_file_sub_boxes(self, box, var, timepoint, single_file_boxes):\n",
    "        db_files = self.get_files_for_corner_points(box[0], box[1], box[2], var, timepoint)\n",
    "        num_db_files = len(set(db_files))\n",
    "\n",
    "        if num_db_files == 1:\n",
    "            unique_db_file = list(set(db_files))[0]\n",
    "            if unique_db_file in single_file_boxes:\n",
    "                raise Exception(f'{unique_db_file} is already in single_file_boxes')\n",
    "            \n",
    "            # stores the minLim of the box for use later when reading in the data.\n",
    "            box_info = self.get_file_for_point(box[0][0], box[1][0], box[2][0], var, timepoint)\n",
    "            box_minLim = box_info[3]\n",
    "            \n",
    "            single_file_boxes[unique_db_file] = (box, box_minLim)\n",
    "            \n",
    "            return\n",
    "        elif db_files[0] != db_files[1]:\n",
    "            # this means that the x_range was sufficiently large such that all of the points were\n",
    "            # not contained in a singular database file.  i.e. the database files were different for\n",
    "            # corners 1 and 2.  the data x_range will now be recursively split in half to find the first databse file endpoint\n",
    "            # along this axis.\n",
    "\n",
    "            # this value is specified as 0 because the x-axis index is 0.  this is used for determing which \n",
    "            # point (X, Y, or Z) the midpoint is going to be tested for.  in this case, this section of code\n",
    "            # is adjusting only the x-axis.\n",
    "            axis_position = 0\n",
    "            # stores the c1 corner point (X, Y, Z) of the box to be used for finding the first box end point\n",
    "            # when shrinking the x-axis into sub-boxes.\n",
    "            datapoint = [box[0][0], box[1][0], box[2][0]]\n",
    "            # which axis is sub-divided, in this case it is the x-axis.\n",
    "            axis_range = list(box[0])\n",
    "            # determine where the end x-axis point is for the first sub-box.\n",
    "            first_box_end_point = self.find_sub_box_end_point(axis_range, datapoint, axis_position, db_files[0], \\\n",
    "                                                                       var, timepoint)\n",
    "\n",
    "            first_sub_box = [[box[0][0], first_box_end_point], box[1], box[2]]\n",
    "            second_sub_box = [[first_box_end_point + 1, box[0][1]], box[1], box[2]]\n",
    "            \n",
    "            sub_boxes = []\n",
    "            sub_boxes.append(first_sub_box)\n",
    "            sub_boxes.append(second_sub_box)\n",
    "            \n",
    "            for sub_box in sub_boxes:\n",
    "                self.recursive_single_database_file_sub_boxes(sub_box, var, timepoint, single_file_boxes)\n",
    "        elif db_files[0] != db_files[2]:\n",
    "            # this means that the y_range was sufficiently large such that all of the points were\n",
    "            # not contained in a singular database file.  i.e. the database files were different for\n",
    "            # corners 1 and 4.  the data y_range will now be recursively split in half to find the first databse file endpoint\n",
    "            # along this axis.\n",
    "\n",
    "            # this value is specified as 1 because the y-axis index is 1.  this is used for determing which \n",
    "            # point (X, Y, or Z) the midpoint is going to be tested for.  in this case, this section of code\n",
    "            # is adjusting only the y-axis.\n",
    "            axis_position = 1\n",
    "            # stores the c1 corner point (X, Y, Z) of the box to be used for finding the first box end point \n",
    "            # when shrinking the y-axis into sub-boxes.\n",
    "            datapoint = [box[0][0], box[1][0], box[2][0]]\n",
    "            # which axis is sub-divided, in this case it is the y-axis.\n",
    "            axis_range = list(box[1])\n",
    "            # determine where the end y-axis point is for the first sub-box.\n",
    "            first_box_end_point = self.find_sub_box_end_point(axis_range, datapoint, axis_position, db_files[0], \\\n",
    "                                                                       var, timepoint)\n",
    "\n",
    "            first_sub_box = [box[0], [box[1][0], first_box_end_point], box[2]]\n",
    "            second_sub_box = [box[0], [first_box_end_point + 1, box[1][1]], box[2]]\n",
    "\n",
    "            sub_boxes = []\n",
    "            sub_boxes.append(first_sub_box)\n",
    "            sub_boxes.append(second_sub_box)\n",
    "            \n",
    "            for sub_box in sub_boxes:\n",
    "                self.recursive_single_database_file_sub_boxes(sub_box, var, timepoint, single_file_boxes)\n",
    "        elif db_files[0] != db_files[3]:\n",
    "            # this means that the z_range was sufficiently large such that all of the points were\n",
    "            # not contained in a singular database file.  i.e. the database files were different for\n",
    "            # corners 1 and 5.  the data z_range will now be recursively split in half to find the first databse file endpoint\n",
    "            # along this axis.\n",
    "\n",
    "            # this value is specified as 2 because the z-axis index is 2.  this is used for determing which \n",
    "            # point (X, Y, or Z) the midpoint is going to be tested for.  in this case, this section of code\n",
    "            # is adjusting only the z-axis.\n",
    "            axis_position = 2\n",
    "            # stores the c1 corner point (X, Y, Z) of the box to be used for finding the first box end point \n",
    "            # when shrinking the z-axis into sub-boxes.\n",
    "            datapoint = [box[0][0], box[1][0], box[2][0]]\n",
    "            # which axis is sub-divided, in this case it is the z-axis.\n",
    "            axis_range = list(box[2])\n",
    "            # determine where the end z-axis point is for the first sub-box.\n",
    "            first_box_end_point = self.find_sub_box_end_point(axis_range, datapoint, axis_position, db_files[0], \\\n",
    "                                                          var, timepoint)\n",
    "\n",
    "            first_sub_box = [box[0], box[1], [box[2][0], first_box_end_point]]\n",
    "            second_sub_box = [box[0], box[1], [first_box_end_point + 1, box[2][1]]]\n",
    "            \n",
    "            sub_boxes = []\n",
    "            sub_boxes.append(first_sub_box)\n",
    "            sub_boxes.append(second_sub_box)\n",
    "            \n",
    "            for sub_box in sub_boxes:\n",
    "                self.recursive_single_database_file_sub_boxes(sub_box, var, timepoint, single_file_boxes)\n",
    "    \n",
    "    def identify_single_database_file_sub_boxes(self, x_range, y_range, z_range, var, timepoint):\n",
    "        # initially assumes the user specified box contains points in different files. the boxes will be split up until all the points\n",
    "        # in each box are from a single database file.\n",
    "        box = [x_range, y_range, z_range]\n",
    "        single_file_boxes = {}\n",
    "        self.recursive_single_database_file_sub_boxes(box, var, timepoint, single_file_boxes)\n",
    "            \n",
    "        return single_file_boxes\n",
    "    \n",
    "    def boxes_contained(self, sub_box, user_box):\n",
    "        contained = False\n",
    "        # checks if the sub-divided box is fully contained within the user-specified box.\n",
    "        if (sub_box[0][0] >= user_box[0][0] and sub_box[0][1] <= user_box[0][1]) and \\\n",
    "            (sub_box[1][0] >= user_box[1][0] and sub_box[1][1] <= user_box[1][1]) and \\\n",
    "            (sub_box[2][0] >= user_box[2][0] and sub_box[2][1] <= user_box[2][1]):\n",
    "            contained = True\n",
    "        \n",
    "        return contained\n",
    "    \n",
    "    def boxes_overlap(self, sub_box, user_box):\n",
    "        overlap = False\n",
    "        # checks if the sub-divided box and the user-specified box overlap on all 3 axes\n",
    "        if (sub_box[0][0] <= user_box[0][1] and user_box[0][0] <= sub_box[0][1]) and \\\n",
    "            (sub_box[1][0] <= user_box[1][1] and user_box[1][0] <= sub_box[1][1]) and \\\n",
    "            (sub_box[2][0] <= user_box[2][1] and user_box[2][0] <= sub_box[2][1]):\n",
    "            overlap = True\n",
    "            \n",
    "        return overlap\n",
    "    \n",
    "    def determine_min_overlap_point(self, voxel, user_box, axis):\n",
    "        min_point = None\n",
    "        \n",
    "        # checks if the user-specified box minimum value along the given axis is <= the voxel minimum value along the same axis.  if so, then the minimum\n",
    "        # value is stored as voxel minimum value.  otherwise, the minimum value is stored as the user-specified box minimum value.\n",
    "        if user_box[axis][0] <= voxel[axis][0]:\n",
    "            min_point = voxel[axis][0]\n",
    "        else:\n",
    "            min_point = user_box[axis][0]\n",
    "            \n",
    "        return min_point\n",
    "    \n",
    "    def determine_max_overlap_point(self, voxel, user_box, axis):\n",
    "        max_point = None\n",
    "        \n",
    "        # checks if the user-specified box maximum value along the given axis is >= the voxel maximum value along the same axis.  if so, then the maximum\n",
    "        # value is stored as voxel maximum value.  otherwise, the maximum value is stored as the user-specified box maximum value.\n",
    "        if user_box[axis][1] >= voxel[axis][1]:\n",
    "            max_point = voxel[axis][1]\n",
    "        else:\n",
    "            max_point = user_box[axis][1]\n",
    "            \n",
    "        return max_point\n",
    "    \n",
    "    def voxel_ranges_in_user_box(self, voxel, user_box):\n",
    "        # determine the minimum and maximum values of the overlap, along each axis, between voxel and the user-specified box for a partially overlapped voxel.\n",
    "        # axis 0 corresponds to the x-axis.\n",
    "        # axis 1 corresponds to the y-axis.\n",
    "        # axis 2 corresponds to the z-axis.\n",
    "        voxel_x_min = self.determine_min_overlap_point(voxel, user_box, axis = 0)\n",
    "        voxel_x_max = self.determine_max_overlap_point(voxel, user_box, axis = 0)\n",
    "        \n",
    "        voxel_y_min = self.determine_min_overlap_point(voxel, user_box, axis = 1)\n",
    "        voxel_y_max = self.determine_max_overlap_point(voxel, user_box, axis = 1)\n",
    "        \n",
    "        voxel_z_min = self.determine_min_overlap_point(voxel, user_box, axis = 2)\n",
    "        voxel_z_max = self.determine_max_overlap_point(voxel, user_box, axis = 2)\n",
    "        \n",
    "        voxel_data = [[voxel_x_min, voxel_x_max], [voxel_y_min, voxel_y_max], [voxel_z_min, voxel_z_max]]\n",
    "        \n",
    "        return voxel_data\n",
    "        \n",
    "    def recursive_sub_boxes_in_file(self, box, user_db_box, morton_voxels_to_read, voxel_side_length = 8):\n",
    "        # recursively sub-divides the database file cube until the entire user-specified box is mapped by morton cubes.\n",
    "        box_x_range = box[0]\n",
    "        box_y_range = box[1]\n",
    "        box_z_range = box[2]\n",
    "        \n",
    "        # only need to check one axes since each sub-box is a cube. this value will be compared to voxel_side_length to limit the recursive\n",
    "        # shrinking algorithm.\n",
    "        sub_box_axes_length = box_x_range[1] - box_x_range[0] + 1\n",
    "        \n",
    "        # checks if the sub-box corner points are all inside the portion of the user-specified box in the database file.\n",
    "        box_fully_contained = self.boxes_contained(box, user_db_box)\n",
    "        box_partially_contained = self.boxes_overlap(box, user_db_box)\n",
    "        # recursively shrinks to voxel-sized boxes (8 x 8 x 8), and stores all of the necessary information regarding these boxes\n",
    "        # that will be used when reading in data from the database file. if a sub-box is fully inside the user-box before getting down \n",
    "        # to the voxel level, then the information is stored for this box without shrinking further to save time.\n",
    "        if box_fully_contained:\n",
    "            # converts the box (X, Y, Z) minimum and maximum points to morton indices for the database file.\n",
    "            morton_index_min = self.mortoncurve.pack(box[0][0], box[1][0], box[2][0])\n",
    "            morton_index_max = self.mortoncurve.pack(box[0][1], box[1][1], box[2][1])\n",
    "            \n",
    "            # calculate the number of voxel lengths that fit along each axis, and then from these values calculate the total number\n",
    "            # of voxels that are inside of this sub-box. each of these values should be an integer because all of the values are divisible.\n",
    "            num_voxels_x = int((box[0][1] - box[0][0] + 1) / voxel_side_length)\n",
    "            num_voxels_y = int((box[1][1] - box[1][0] + 1) / voxel_side_length)\n",
    "            num_voxels_z = int((box[2][1] - box[2][0] + 1) / voxel_side_length)\n",
    "            num_voxels = num_voxels_x * num_voxels_y * num_voxels_z\n",
    "            \n",
    "            # whether the voxel type is fully contained in the user-box ('f') or partially contained ('p') for each voxel.\n",
    "            voxel_type = ['f'] * num_voxels\n",
    "            \n",
    "            # stores the morton indices for reading from the database file efficiently and voxel type information for parsing out the voxel information.\n",
    "            if morton_voxels_to_read == list():\n",
    "                morton_voxel_info = [[morton_index_min, morton_index_max], num_voxels, voxel_type]\n",
    "                morton_voxels_to_read.append(morton_voxel_info)\n",
    "            else:\n",
    "                # check if the most recent sub-box maximum is 1 index less than the new sub-box minimum.  if so, then \n",
    "                # extend the range of the previous sub-box morton maximum to stitch these two boxes together. also, add the number of voxels \n",
    "                # and append the new voxel type information so that the voxel boundaries can be parsed correctly when reading the data.\n",
    "                if morton_voxels_to_read[-1][0][1] == (morton_index_min - 1):\n",
    "                    morton_voxels_to_read[-1][0][1] = morton_index_max\n",
    "                    morton_voxels_to_read[-1][1] += num_voxels\n",
    "                    morton_voxels_to_read[-1][2] += voxel_type\n",
    "                else:\n",
    "                    # start a new morton sequence that will be read in separately.\n",
    "                    morton_voxel_info = [[morton_index_min, morton_index_max], num_voxels, voxel_type]\n",
    "                    morton_voxels_to_read.append(morton_voxel_info)\n",
    "                    \n",
    "            return\n",
    "        elif sub_box_axes_length == voxel_side_length:\n",
    "            if box_fully_contained or box_partially_contained:\n",
    "                # converts the box (X, Y, Z) minimum and maximum points to morton indices for the database file.\n",
    "                morton_index_min = self.mortoncurve.pack(box[0][0], box[1][0], box[2][0])\n",
    "                morton_index_max = self.mortoncurve.pack(box[0][1], box[1][1], box[2][1])\n",
    "                \n",
    "                # calculate the number of voxel lengths that fit along each axis, and then from these values calculate the total number\n",
    "                # of voxels that are inside of this sub-box. each of these values should be an integer because all of the values are divisible.\n",
    "                num_voxels_x = int((box[0][1] - box[0][0] + 1) / voxel_side_length)\n",
    "                num_voxels_y = int((box[1][1] - box[1][0] + 1) / voxel_side_length)\n",
    "                num_voxels_z = int((box[2][1] - box[2][0] + 1) / voxel_side_length)\n",
    "                num_voxels = num_voxels_x * num_voxels_y * num_voxels_z\n",
    "                \n",
    "                # whether the voxel type is fully contained in the user-box ('f') or partially contained ('p') for each voxel.\n",
    "                voxel_type = 'p'\n",
    "                if box_fully_contained:\n",
    "                    voxel_type = 'f'\n",
    "                \n",
    "                voxel_type = [voxel_type] * num_voxels\n",
    "                \n",
    "                # stores the morton indices for reading from the database file efficiently and voxel info for parsing out the voxel information.\n",
    "                if morton_voxels_to_read == list():\n",
    "                    morton_voxel_info = [[morton_index_min, morton_index_max], num_voxels, voxel_type]\n",
    "                    morton_voxels_to_read.append(morton_voxel_info)\n",
    "                else:\n",
    "                    # check if the most recent sub-box maximum is 1 index less than the new sub-box minimum.  if so, then \n",
    "                    # extend the range of the previous sub-box morton maximum to stitch these two boxes together. also, add the number of voxels \n",
    "                    # and append the new voxel type information so that the voxel boundaries can be parsed correctly when reading the data.\n",
    "                    if morton_voxels_to_read[-1][0][1] == (morton_index_min - 1):\n",
    "                        morton_voxels_to_read[-1][0][1] = morton_index_max\n",
    "                        morton_voxels_to_read[-1][1] += num_voxels\n",
    "                        morton_voxels_to_read[-1][2] += voxel_type\n",
    "                    else:\n",
    "                        # start a new morton sequence that will be read in separately.\n",
    "                        morton_voxel_info = [[morton_index_min, morton_index_max], num_voxels, voxel_type]\n",
    "                        morton_voxels_to_read.append(morton_voxel_info)\n",
    "\n",
    "            return\n",
    "        else:\n",
    "            if box_partially_contained:\n",
    "                # sub-divide the box into 8 sub-cubes (divide the x-, y-, and z- axes in half) and recursively check each box if \n",
    "                # it is inside the user-specified box, if necessary.\n",
    "                box_x_range_midpoint = math.floor((box_x_range[0] + box_x_range[1]) / 2)\n",
    "                box_y_range_midpoint = math.floor((box_y_range[0] + box_y_range[1]) / 2)\n",
    "                box_z_range_midpoint = math.floor((box_z_range[0] + box_z_range[1]) / 2)\n",
    "                \n",
    "                # ordering sub-boxes 1-8 below in this order maintains the morton-curve index structure, such that \n",
    "                # the minimum (X, Y, Z) morton index for a new box only needs to be compared to the last \n",
    "                # sub-boxes' maximum (X, Y, Z) morton index to see if they can be stitched together.\n",
    "                \n",
    "                # new_sub_box_1 is the sub-box bounded by [x_min, x_midpoint], [y_min, y_midpoint], and [z_min, z_midpoint]\n",
    "                # new_sub_box_2 is the sub-box bounded by [x_midpoint + 1, x_max], [y_min, y_midpoint], and [z_min, z_midpoint]\n",
    "                # new_sub_box_3 is the sub-box bounded by [x_min, x_midpoint], [y_midpoint + 1, y_max], and [z_min, z_midpoint]\n",
    "                # new_sub_box_4 is the sub-box bounded by [x_midpoint + 1, x_max], [y_midpoint + 1, y_max], and [z_min, z_midpoint]\n",
    "                new_sub_box_1 = [[box_x_range[0], box_x_range_midpoint], [box_y_range[0], box_y_range_midpoint], [box_z_range[0], box_z_range_midpoint]]\n",
    "                new_sub_box_2 = [[box_x_range_midpoint + 1, box_x_range[1]], [box_y_range[0], box_y_range_midpoint], [box_z_range[0], box_z_range_midpoint]]\n",
    "                new_sub_box_3 = [[box_x_range[0], box_x_range_midpoint], [box_y_range_midpoint + 1, box_y_range[1]], [box_z_range[0], box_z_range_midpoint]]\n",
    "                new_sub_box_4 = [[box_x_range_midpoint + 1, box_x_range[1]], [box_y_range_midpoint + 1, box_y_range[1]], [box_z_range[0], box_z_range_midpoint]]\n",
    "                \n",
    "                # new_sub_box_5 is the sub-box bounded by [x_min, x_midpoint], [y_min, y_midpoint], and [z_midpoint + 1, z_max]\n",
    "                # new_sub_box_6 is the sub-box bounded by [x_midpoint + 1, x_max], [y_min, y_midpoint], and [z_midpoint + 1, z_max]\n",
    "                # new_sub_box_7 is the sub-box bounded by [x_min, x_midpoint], [y_midpoint + 1, y_max], and [z_midpoint + 1, z_max]\n",
    "                # new_sub_box_8 is the sub-box bounded by [x_midpoint + 1, x_max], [y_midpoint + 1, y_max], and [z_midpoint + 1, z_max]\n",
    "                new_sub_box_5 = [[box_x_range[0], box_x_range_midpoint], [box_y_range[0], box_y_range_midpoint], [box_z_range_midpoint + 1, box_z_range[1]]]\n",
    "                new_sub_box_6 = [[box_x_range_midpoint + 1, box_x_range[1]], [box_y_range[0], box_y_range_midpoint], [box_z_range_midpoint + 1, box_z_range[1]]]\n",
    "                new_sub_box_7 = [[box_x_range[0], box_x_range_midpoint], [box_y_range_midpoint + 1, box_y_range[1]], [box_z_range_midpoint + 1, box_z_range[1]]]\n",
    "                new_sub_box_8 = [[box_x_range_midpoint + 1, box_x_range[1]], [box_y_range_midpoint + 1, box_y_range[1]], [box_z_range_midpoint + 1, box_z_range[1]]]\n",
    "\n",
    "                new_sub_boxes = []\n",
    "                new_sub_boxes.append(new_sub_box_1)\n",
    "                new_sub_boxes.append(new_sub_box_2)\n",
    "                new_sub_boxes.append(new_sub_box_3)\n",
    "                new_sub_boxes.append(new_sub_box_4)\n",
    "                new_sub_boxes.append(new_sub_box_5)\n",
    "                new_sub_boxes.append(new_sub_box_6)\n",
    "                new_sub_boxes.append(new_sub_box_7)\n",
    "                new_sub_boxes.append(new_sub_box_8)\n",
    "                    \n",
    "                for new_sub_box in new_sub_boxes:\n",
    "                    # checks if a sub-box is at least partially contained inside the user-specified box. if so, then the sub-box will \n",
    "                    # be recursively searched until an entire sub-box is inside the user-specified box.\n",
    "                    new_sub_box_partially_contained = self.boxes_overlap(new_sub_box, user_db_box)\n",
    "\n",
    "                    if new_sub_box_partially_contained:\n",
    "                        self.recursive_sub_boxes_in_file(new_sub_box, user_db_box, morton_voxels_to_read, voxel_side_length)\n",
    "        return\n",
    "        \n",
    "    def identify_sub_boxes_in_file(self, user_db_box, var, timepoint, voxel_side_length = 8):\n",
    "        # initially assumes the user-specified box in the file is not the entire box representing the file. the database file box will \n",
    "        # be sub-divided into morton cubes until the user-specified box is completely mapped by all of these sub-cubes.\n",
    "        user_db_box_x_range = user_db_box[0]\n",
    "        user_db_box_y_range = user_db_box[1]\n",
    "        user_db_box_z_range = user_db_box[2]\n",
    "        \n",
    "        user_db_box_x_min = user_db_box_x_range[0]\n",
    "        user_db_box_y_min = user_db_box_y_range[0]\n",
    "        user_db_box_z_min = user_db_box_z_range[0]\n",
    "        \n",
    "        # retrieve the morton index limits (minLim, maxLim) of the cube representing the whole database file\n",
    "        f, cornercode, offset, minLim, maxLim = self.get_file_for_point(user_db_box_x_min, user_db_box_y_min, user_db_box_z_min, var, timepoint)\n",
    "        minLim_xyz = self.mortoncurve.unpack(minLim)\n",
    "        maxLim_xyz = self.mortoncurve.unpack(maxLim)\n",
    "        \n",
    "        # get the box for the entire database file so that it can be recursively broken down into cubes\n",
    "        db_box = [[minLim_xyz[0], maxLim_xyz[0]], [minLim_xyz[1], maxLim_xyz[1]], [minLim_xyz[2], maxLim_xyz[2]]]\n",
    "        \n",
    "        # these are the constituent file sub-cubes that make up the part of the user-specified box in the database file\n",
    "        morton_voxels_to_read = []\n",
    "        self.recursive_sub_boxes_in_file(db_box, user_db_box, morton_voxels_to_read, voxel_side_length)\n",
    "\n",
    "        return morton_voxels_to_read\n",
    "        \n",
    "    def get_velocities_for_all_points(self, x_range, y_range, z_range, min_step = 1):\n",
    "        # manually retrieves the velocities for all points inside the box. this is computationally expensive and not efficient, and \n",
    "        # this function is deprecated.\n",
    "        x_min = x_range[0]; x_max = x_range[1];\n",
    "        y_min = y_range[0]; y_max = y_range[1];\n",
    "        z_min = z_range[0]; z_max = z_range[1];\n",
    "        \n",
    "        current_x_max = x_max\n",
    "        current_y_max = y_max\n",
    "        current_z_max = z_max\n",
    "        \n",
    "        velocity_map = {}\n",
    "        velocity_data = np.array([-1, -1, -1])\n",
    "        for x_point in np.arange(x_min, x_max + 1, min_step):\n",
    "            for y_point in np.arange(y_min, y_max + 1, min_step):\n",
    "                for z_point in np.arange(z_min, z_max + 1, min_step):\n",
    "                    velocity_data = self.getISO_Point(x_point, y_point, z_point, var = 'vel', timepoint = 0, verbose = False)\n",
    "                    \n",
    "                    velocity_map[(x_point, y_point, z_point)] = velocity_data\n",
    "                    #print(x_point)\n",
    "                    #print(cornercode, offset)\n",
    "        \n",
    "        return velocity_map\n",
    "        \n",
    "    def get_offset(self, X, Y, Z):\n",
    "        \"\"\"\n",
    "        TODO is this code correct for velocity as well?  YES\n",
    "        \"\"\"\n",
    "        # morton curve index corresponding to the user specified X, Y, and Z values\n",
    "        code = self.mortoncurve.pack(X, Y, Z)\n",
    "        # always looking at an 8 x 8 x 8 box around the grid point, so the shift is always 9 bits to determine \n",
    "        # the bottom left corner of the box. the cornercode (bottom left corner of the 8 x 8 x 8 box) is always \n",
    "        # in the same file as the user-specified grid point.\n",
    "        # equivalent to 512 * (math.floor(code / 512))\n",
    "        cornercode = (code >> 9) << 9\n",
    "        corner = np.array(self.mortoncurve.unpack(cornercode))\n",
    "        # calculates the offset between the grid point and corner of the box and converts it to a 4-byte float.\n",
    "        offset = np.sum((np.array([X, Y, Z]) - corner) * np.array([1, 8, 64]))\n",
    "        \n",
    "        return cornercode, offset\n",
    "    \n",
    "    def get_file_for_point(self, X, Y, Z, var = 'pr', timepoint = 0):\n",
    "        \"\"\"\n",
    "        querying the cached SQL metadata for the file for the user specified grid point\n",
    "        \"\"\"\n",
    "        cornercode, offset = self.get_offset(X, Y, Z)\n",
    "        t = self.cache[(self.cache['minLim'] <= cornercode) & (self.cache['maxLim'] >= cornercode)]\n",
    "        t = t.iloc[0]\n",
    "        dataN = t.path.split(\"/\")\n",
    "        f = f'/home/idies/workspace/turb/data{t.ProductionMachineName[-2:]}_{dataN[2][-2:]}/{dataN[-1]}/{t.ProductionDatabaseName}_{var}_{timepoint}.bin'\n",
    "        return f, cornercode, offset, t.minLim, t.maxLim\n",
    "        \n",
    "    def get_iso_points(self, db_file, morton_voxels_to_read, output_data, \\\n",
    "                      db_minLim, x_min, y_min, z_min, x_range, y_range, z_range, \\\n",
    "                      num_values_per_datapoint = 1, bytes_per_datapoint = 4, voxel_side_length = 8, verbose = False):\n",
    "        \"\"\"\n",
    "        retrieve the values for the specified var(iable) in the user-specified box and at the specified timepoint.\n",
    "        \"\"\"\n",
    "        # used to check the memory usage so that it could be minimized.\n",
    "        #process = psutil.Process(os.getpid())\n",
    "        #print(f'memory usage 1 (gigabytes) = {(process.memory_info().rss) / (1024**3)}')  # in bytes. \n",
    "\n",
    "        # volume of the voxel cube.\n",
    "        voxel_cube_size = voxel_side_length**3\n",
    "        \n",
    "        # iterates over the groups of morton adjacent voxels to minimize the number I/O operations when reading the data.\n",
    "        for morton_data in morton_voxels_to_read:\n",
    "            # the continuous range of morton indices compiled from adjacent voxels that can be read in from the file at the same time.\n",
    "            morton_index_range = morton_data[0]\n",
    "            # the voxels that will be parsed out from the data that is read in. the voxels need to be parsed separately because the data is sequentially\n",
    "            # ordered within a voxel as opposed to morton ordered outside a voxel.\n",
    "            num_voxels = morton_data[1]\n",
    "            \n",
    "            # morton_index_min is equivalent to \"cornercode + offset\" because morton_index_min is defined as the corner of a voxel.\n",
    "            morton_index_min = morton_index_range[0]\n",
    "            morton_index_max = morton_index_range[1]\n",
    "            morton_index_diff = (morton_index_max - morton_index_min) + 1\n",
    "            \n",
    "            # the point to seek to in order to start reading the file for this morton index range.\n",
    "            seek_distance = num_values_per_datapoint * bytes_per_datapoint * (morton_index_min - db_minLim)\n",
    "            # number of bytes to read in from the database file.\n",
    "            read_length = num_values_per_datapoint * bytes_per_datapoint * morton_index_diff\n",
    "            \n",
    "            # read the data. this method is deprecated because it is slow.\n",
    "            #with open(db_file, 'rb') as b:\n",
    "            #    b.seek(seek_distance)\n",
    "            #    xraw = b.read(read_length)\n",
    "            #\n",
    "            # unpack the data as 4-byte floats.\n",
    "            #l = struct.unpack('f' * num_values_per_datapoint * morton_index_diff, xraw)\n",
    "            \n",
    "            # used to check the memory usage so that it could be minimized.\n",
    "            #print(f'memory usage 1a (gigabytes) = {(process.memory_info().rss) / (1024**3)}')  # in bytes. \n",
    "            \n",
    "            # read the data efficiently.\n",
    "            l = np.fromfile(db_file, dtype = 'f', count = read_length, offset = seek_distance)\n",
    "            l = l[np.arange(0, l.size - num_values_per_datapoint + 1, num_values_per_datapoint)[:, None] + np.arange(num_values_per_datapoint)]\n",
    "            \n",
    "            # used to check the memory usage so that it could be minimized.\n",
    "            #print(f'memory usage 1b (gigabytes) = {(process.memory_info().rss) / (1024**3)}')  # in bytes. \n",
    "            \n",
    "            # iterate over each voxel in voxel_data.\n",
    "            for voxel_count in np.arange(0, num_voxels, 1):\n",
    "                # get the origin point (X, Y, Z) for the voxel.\n",
    "                voxel_origin_point = self.mortoncurve.unpack(morton_index_min + (voxel_count * voxel_cube_size))\n",
    "                \n",
    "                # voxel axes ranges.\n",
    "                voxel_ranges = [[voxel_origin_point[0], voxel_origin_point[0] + voxel_side_length - 1], \\\n",
    "                                [voxel_origin_point[1], voxel_origin_point[1] + voxel_side_length - 1], \\\n",
    "                                [voxel_origin_point[2], voxel_origin_point[2] + voxel_side_length - 1]]\n",
    "                \n",
    "                # assumed to be a voxel that is fully inside the user-specified box. if the box was only partially contained inside the user-specified\n",
    "                # box, then the voxel ranges are corrected to the edges of the user-specified box.\n",
    "                voxel_type = morton_data[2][voxel_count]\n",
    "                if voxel_type == 'p':\n",
    "                    voxel_ranges = self.voxel_ranges_in_user_box(voxel_ranges, [x_range, y_range, z_range])\n",
    "                \n",
    "                # retrieve the x-, y-, and z-ranges for the voxel.\n",
    "                voxel_x_range = voxel_ranges[0]\n",
    "                voxel_y_range = voxel_ranges[1]\n",
    "                voxel_z_range = voxel_ranges[2]\n",
    "                \n",
    "                # pull out the data that corresponds to this voxel.\n",
    "                sub_l_array = l[voxel_count * voxel_cube_size : (voxel_count + 1) * voxel_cube_size]\n",
    "                \n",
    "                # reshape the sub_l array into a voxel matrix.\n",
    "                sub_l_array = sub_l_array.reshape(voxel_side_length, voxel_side_length, voxel_side_length, num_values_per_datapoint)\n",
    "                # swap the x- and z- axes to maintain the correct structure. turned this off to leave as (Z, Y, X) for hdf5 file format.\n",
    "                #sub_l_array = np.swapaxes(sub_l_array, 0, 2)\n",
    "                # remove parts of the voxel that are outside of the user-specified box.\n",
    "                sub_l_array = sub_l_array[voxel_z_range[0] % voxel_side_length : (voxel_z_range[1] % voxel_side_length) + 1, \\\n",
    "                                          voxel_y_range[0] % voxel_side_length : (voxel_y_range[1] % voxel_side_length) + 1, \\\n",
    "                                          voxel_x_range[0] % voxel_side_length : (voxel_x_range[1] % voxel_side_length) + 1]\n",
    "                \n",
    "                # insert sub_l_array into output_data.\n",
    "                output_data[voxel_z_range[0] - z_min : voxel_z_range[1] - z_min + 1, \\\n",
    "                            voxel_y_range[0] - y_min : voxel_y_range[1] - y_min + 1, \\\n",
    "                            voxel_x_range[0] - x_min : voxel_x_range[1] - x_min + 1] = np.array(sub_l_array)\n",
    "                \n",
    "                # clear sub_l_array to free up memory.\n",
    "                sub_l_array = None\n",
    "                \n",
    "            # clear l to free up memory.\n",
    "            l = None\n",
    "        \n",
    "        # used to check the memory usage so that it could be minimized.\n",
    "        #print(f'memory usage 2 (gigabytes) = {(process.memory_info().rss) / (1024**3)}')  # in bytes. \n",
    "            \n",
    "    def get_iso_point_original(self, X, Y, Z, var = 'pr', timepoint = 0, verbose = False):\n",
    "        \"\"\"\n",
    "        find the value for the specified var(iable) at the specified point XYZ and specified time.\n",
    "        Position is assumed to be a point of the grid, i.e. should be integers, and time should be an integer between 0 and 5.\n",
    "        \"\"\"\n",
    "        f, cornercode, offset, minLim, maxLim = self.get_file_for_point(X, Y, Z, var, timepoint)\n",
    "        if verbose:\n",
    "            print(f'filename : {f}')\n",
    "            print(f'cornercode : {cornercode}')\n",
    "            print(f'corner : {np.array(self.mortoncurve.unpack(cornercode))}')\n",
    "            print(f'offset : {offset}')\n",
    "            print(f'minLim : {minLim}')\n",
    "            print(f'maxLim : {maxLim}')\n",
    "            #print(f, cornercode, offset, minLim, maxLim)\n",
    "        \n",
    "        N = 1\n",
    "        if var == 'vel':\n",
    "            N = 3\n",
    "        \n",
    "        with open(f, 'rb') as b:\n",
    "            b.seek(N * 4 * (cornercode + offset - minLim))\n",
    "            xraw = b.read(4 * N)\n",
    "        \n",
    "        l = struct.unpack('f' * N, xraw)\n",
    "        \n",
    "        return l\n",
    "    \n",
    "    def write_output_matrix_to_hdf5(self, output_data, output_path, output_filename, dataset_name):\n",
    "        # write output_data to a hdf5 file.\n",
    "        with h5py.File(output_path + output_filename + '.h5', 'w') as h5f:\n",
    "            h5f.create_dataset(dataset_name, data = output_data)\n",
    "    \n",
    "\"\"\"\n",
    "driver functions for processing the data and retrieving the data values for all points inside of a user-specified box.\n",
    "\"\"\"\n",
    "def convert_to_0_based_value(value):\n",
    "    # convert the 1-based value to a 0-based value.\n",
    "    updated_value = value - 1\n",
    "    \n",
    "    return updated_value\n",
    "\n",
    "def convert_to_0_based_ranges(x_range, y_range, z_range):\n",
    "    # convert the 1-based axes ranges to 0-based axes ranges.\n",
    "    updated_x_range = list(np.array(x_range) - 1)\n",
    "    updated_y_range = list(np.array(y_range) - 1)\n",
    "    updated_z_range = list(np.array(z_range) - 1)\n",
    "    \n",
    "    return updated_x_range, updated_y_range, updated_z_range\n",
    "    \n",
    "def retrieve_data_for_point(X, Y, Z, output_data, x_range, y_range, z_range):\n",
    "    # finds the indices corresponding the to the (X, Y, Z) datapoint that the user is asking for and returns the stored data.\n",
    "    # minimum values along each axis for the user-specified box.\n",
    "    x_min = x_range[0]\n",
    "    y_min = y_range[0]\n",
    "    z_min = z_range[0]\n",
    "\n",
    "    # maximum values along each axis for the user-specified box.\n",
    "    x_max = x_range[1]\n",
    "    y_max = y_range[1]\n",
    "    z_max = z_range[1]\n",
    "\n",
    "    # checks if the X, Y, and Z datapoints are inside of the user-specified box that data was retrieved for.\n",
    "    if not (x_min <= X <= x_max):\n",
    "        raise IndexError(f'X datapoint, {X}, must be in the range of [{x_min}, {x_max}]')\n",
    "\n",
    "    if not (y_min <= Y <= y_max):\n",
    "        raise IndexError(f'Y datapoint, {Y}, must be in the range of [{y_min}, {y_max}]')\n",
    "\n",
    "    if not (z_min <= Z <= z_max):\n",
    "        raise IndexError(f'Z datapoint, {Z}, must be in the range of [{z_min}, {z_max}]')\n",
    "\n",
    "    # converts the X, Y, and Z datapoints to their corresponding indices in the output_data array.\n",
    "    x_index = X - x_min\n",
    "    y_index = Y - y_min\n",
    "    z_index = Z - z_min\n",
    "\n",
    "    # retrieves the values stored in the output_data array for the (X, Y, Z) datapoint.\n",
    "    # note: output_data is ordered as (Z, Y, X).\n",
    "    data_value = output_data[z_index][y_index][x_index]\n",
    "\n",
    "    return data_value\n",
    "    \n",
    "def process_data(cube_num, cube_dimensions, cube_title, output_path, x_range, y_range, z_range, var, timepoint):\n",
    "    # calculate how much time it takes to run the code.\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # starting the tracemalloc library.\n",
    "    #tracemalloc.start()\n",
    "    # checking the memory usage of the program.\n",
    "    #tracemem_start = [mem_value / (1024**3) for mem_value in tracemalloc.get_traced_memory()]\n",
    "    #tracemem_used_start = tracemalloc.get_tracemalloc_memory() / (1024**3)\n",
    "\n",
    "    # gets velocity for all points inside the user specified box.\n",
    "    iso_data = iso_cube(cube_num = cube_num, cube_dimensions = cube_dimensions, cube_title = cube_title)\n",
    "\n",
    "    # data constants.\n",
    "    # placeholder for missing values that will be used to fill the output_data array when it is initialized.\n",
    "    missing_value_placeholder = -999.9\n",
    "    # bytes per value associated with a datapoint.\n",
    "    bytes_per_datapoint = 4\n",
    "    # maximum data size allowed to be retrieved, in gigabytes (GB).\n",
    "    max_data_size = 3.0\n",
    "    # smallest sub-box size to recursively shrink to. if this size box is only partially contained in the user-specified box, then\n",
    "    # the (X, Y, Z) points outside of the user-specified box will be trimmed.  the value is the length of one side of the cube.\n",
    "    voxel_side_length = 8\n",
    "\n",
    "    # the number of values to read per datapoint. for pressure data this value is 1.  for velocity\n",
    "    # data this value is 3, because there is a velocity measurement along each axis.\n",
    "    num_values_per_datapoint = 1\n",
    "    if var == 'vel':\n",
    "        num_values_per_datapoint = 3\n",
    "\n",
    "    # used for determining the indices in the output array for each X, Y, Z datapoint.\n",
    "    x_min = x_range[0]\n",
    "    y_min = y_range[0]\n",
    "    z_min = z_range[0]\n",
    "\n",
    "    # used for creating the 3-D output array using numpy, and also checking that the user did not request too much data.\n",
    "    x_axis_length = x_range[1] - x_range[0] + 1\n",
    "    y_axis_length = y_range[1] - y_range[0] + 1\n",
    "    z_axis_length = z_range[1] - z_range[0] + 1\n",
    "\n",
    "    # total number of datapoints, used for checking if the user requested too much data.\n",
    "    num_datapoints = x_axis_length * y_axis_length * z_axis_length\n",
    "    # total size of data, in GBs, requested by the user's box.\n",
    "    requested_data_size = (num_datapoints * bytes_per_datapoint * num_values_per_datapoint) / float(1024**3)\n",
    "    # maximum number of datapoints that can be read in. currently set to 3 GBs worth of datapoints.\n",
    "    max_datapoints = int((max_data_size * (1024**3)) / (bytes_per_datapoint * float(num_values_per_datapoint)))\n",
    "    # approximate max size of a cube representing the maximum data points. this number is rounded down.\n",
    "    approx_max_cube = int(max_datapoints**(1/3))\n",
    "\n",
    "    if requested_data_size > max_data_size:\n",
    "        raise ValueError(f'Please specify a box with fewer than {max_datapoints} data points. This represents an approximate cube size ' + \\\n",
    "                         f'of ({approx_max_cube} x {approx_max_cube} x {approx_max_cube}).')\n",
    "\n",
    "    # begin processing of data.\n",
    "    # -----\n",
    "    print('Note: For larger boxes, e.g. 512-cubed and up, processing will take approximately 1 minute or more...\\n' + '-' * 5)\n",
    "    \n",
    "    # -----\n",
    "    # get a map of the database files where all the data points are in.\n",
    "    print('\\nStep 1: Determining which database files the user-specified box is found in...\\n' + '-' * 25)\n",
    "    \n",
    "    # calculate how much time it takes to run step 1.\n",
    "    start_time_step1 = time.perf_counter()\n",
    "\n",
    "    #%time user_single_db_boxes = iso_data.identify_single_database_file_sub_boxes(x_range, y_range, z_range, var, timepoint)\n",
    "    user_single_db_boxes = iso_data.identify_single_database_file_sub_boxes(x_range, y_range, z_range, var, timepoint)\n",
    "\n",
    "    print(f'number of database files that the user-specified box is found in:\\n{len(user_single_db_boxes)}\\n')\n",
    "    # for db_file in sorted(user_single_db_boxes, key = lambda x: os.path.basename(x)):\n",
    "    #     print(db_file)\n",
    "    #     print(user_single_db_boxes[db_file])\n",
    "    \n",
    "    # calculate how much time it takes to run step 1.\n",
    "    end_time_step1 = time.perf_counter()\n",
    "\n",
    "    print('Successfully completed.\\n' + '-' * 5)\n",
    "    \n",
    "    # -----\n",
    "    # recursively break down each single file box into sub-boxes, each of which is exactly one of the sub-divided cubes of the database file.\n",
    "    print('\\nStep 2: Recursively breaking down the portion of the user-specified box in each database file into voxels...\\n' + '-' * 25)\n",
    "    \n",
    "    # calculate how much time it takes to run step 2.\n",
    "    start_time_step2 = time.perf_counter()\n",
    "    \n",
    "    sub_db_boxes = {}\n",
    "    for db_file in sorted(user_single_db_boxes, key = lambda x: os.path.basename(x)):\n",
    "        user_db_box = user_single_db_boxes[db_file][0]\n",
    "\n",
    "        #%time sub_boxes, read_byte_sequences = iso_data.identify_sub_boxes_in_file(user_db_box, var, timepoint, voxel_side_length)\n",
    "        morton_voxels_to_read = iso_data.identify_sub_boxes_in_file(user_db_box, var, timepoint, voxel_side_length)\n",
    "\n",
    "        sub_db_boxes[db_file] = morton_voxels_to_read\n",
    "        \n",
    "    print('sub-box statistics for the database file(s):\\n-')\n",
    "    print(f'minimum number of sub-boxes to read in a database file:\\n{np.min([len(sub_db_boxes[db_file]) for db_file in sub_db_boxes])}')\n",
    "    print(f'maximum number of sub-boxes to read in a database file:\\n{np.max([len(sub_db_boxes[db_file]) for db_file in sub_db_boxes])}\\n')\n",
    "    #for db_file in sorted(sub_db_boxes, key = lambda x: os.path.basename(x)):\n",
    "    #    print(db_file)\n",
    "    #    print(f'number of boxes = {len(sub_db_boxes[db_file])}')\n",
    "    #    #print(sub_db_boxes[db_file])\n",
    "    \n",
    "    # calculate how much time it takes to run step 2.\n",
    "    end_time_step2 = time.perf_counter()\n",
    "    \n",
    "    print('Successfully completed.\\n' + '-' * 5)\n",
    "\n",
    "    # -----\n",
    "    # read the data.\n",
    "    print('\\nStep 3: Reading the data from all of the database files and storing the values into a matrix...\\n' + '-' * 25)\n",
    "    \n",
    "    # calculate how much time it takes to run step 3.\n",
    "    start_time_step3 = time.perf_counter()\n",
    "    \n",
    "    # used to check the memory usage so that it could be minimized.\n",
    "    #process = psutil.Process(os.getpid())\n",
    "    #print(f'memory usage 0 (gigabytes) = {(process.memory_info().rss) / (1024**3)}')  # in bytes.\n",
    "    \n",
    "    # pre-fill the output data 3-d array that will be filled with the data that is read in. initially the datatype is set to \"object\"\n",
    "    # so that the array is filled with \"None\" values. the filled output_data array will be retyped to float ('f'). this has been\n",
    "    # deprecated because if the dtype is specified as a float, then \"None\" is not stored as the placeholder values.  \n",
    "    #output_data = np.empty((z_axis_length, y_axis_length, x_axis_length, num_values_per_datapoint), dtype = 'f')\n",
    "    \n",
    "    # pre-fill the output data 3-d array that will be filled with the data that is read in. initially the datatype is set to \"f\" (float)\n",
    "    # so that the array is filled with the missing placeholder values (-999.9). \n",
    "    output_data = np.full((z_axis_length, y_axis_length, x_axis_length, num_values_per_datapoint), fill_value = missing_value_placeholder, dtype = 'f')\n",
    "    \n",
    "    # iterate over the database files and morton sub-boxes to read the data from.\n",
    "    for db_file in sub_db_boxes:\n",
    "        morton_voxels_to_read = sub_db_boxes[db_file]\n",
    "        db_minLim = user_single_db_boxes[db_file][1]\n",
    "\n",
    "        iso_data.get_iso_points(db_file, morton_voxels_to_read, output_data, \\\n",
    "                               db_minLim, x_min, y_min, z_min, x_range, y_range, z_range, \\\n",
    "                               num_values_per_datapoint, bytes_per_datapoint, voxel_side_length, verbose = False)\n",
    "    \n",
    "    # checks to make sure that data was read in for all points.\n",
    "    if missing_value_placeholder in output_data:\n",
    "        raise Exception(f'output_data was not filled correctly')\n",
    "        \n",
    "    # retyping the datatype for output_data to float ('f') after making sure there were no \"None\" entries left in output_data. this has been\n",
    "    # deprecated because the output_data array is now initialized with missing placeholder values of type \"f\" (float).\n",
    "    #output_data = output_data.astype('f')\n",
    "    \n",
    "    # used to check the memory usage so that it could be minimized.\n",
    "    #print(f'memory usage 3 (gigabytes) = {(process.memory_info().rss) / (1024**3)}')  # in bytes.\n",
    "    \n",
    "    # calculate how much time it takes to run step 3.\n",
    "    end_time_step3 = time.perf_counter()\n",
    "    \n",
    "    print('\\nSuccessfully completed.\\n' + '-' * 5)\n",
    "    \n",
    "    # -----\n",
    "    # write the output file.\n",
    "    print('\\nStep 4: Writing the output matrix to a hdf5 file...\\n' + '-' * 25)\n",
    "    \n",
    "    # calculate how much time it takes to run step 4.\n",
    "    start_time_step4 = time.perf_counter()\n",
    "    \n",
    "    # write output_data to a hdf5 file.\n",
    "    # the output filename specifies the title of the cube, and the x-, y-, and z-ranges so that the file is unique. 1 is added to all of the \n",
    "    # ranges, and the timepoint, because python uses 0-based indices, and the output is desired to be 1-based indices.\n",
    "    output_filename = f'{cube_title}_{var}_t{timepoint + 1}_z{z_range[0] + 1}-{z_range[1] + 1}_y{y_range[0] + 1}-{y_range[1] + 1}_x{x_range[0] + 1}-{x_range[1] + 1}'\n",
    "    # formats the dataset name for the hdf5 output file. \"untitled\" is a placeholder.\n",
    "    dataset_name = 'Untitled'\n",
    "    if var == 'vel':\n",
    "        dataset_name = 'Velocity'\n",
    "    elif var == 'pr':\n",
    "        dataset_name = 'Pressure'\n",
    "        \n",
    "    # adds the timpoint information, formatted with leading zeros out to 1000, to dataset_name. 1 is added to timepoint because python uses\n",
    "    # 0-based indices, and the output is desired to be 1-based indices.\n",
    "    dataset_name += '_' + str(timepoint + 1).zfill(4)\n",
    "    \n",
    "    # writes the output file.\n",
    "    iso_data.write_output_matrix_to_hdf5(output_data, output_path, output_filename, dataset_name)\n",
    "    \n",
    "    # calculate how much time it takes to run step 4.\n",
    "    end_time_step4 = time.perf_counter()\n",
    "    \n",
    "    print('\\nSuccessfully completed.\\n' + '-' * 5)\n",
    "    \n",
    "    # memory used during processing as calculated by tracemalloc.\n",
    "    #tracemem_end = [mem_value / (1024**3) for mem_value in tracemalloc.get_traced_memory()]\n",
    "    #tracemem_used_end = tracemalloc.get_tracemalloc_memory() / (1024**3)\n",
    "    # stopping the tracemalloc library.\n",
    "    #tracemalloc.stop()\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    # see how much memory was used during processing.\n",
    "    # memory used at program start.\n",
    "    #print(f'\\nstarting memory used in GBs [current, peak] = {tracemem_start}')\n",
    "    # memory used by tracemalloc.\n",
    "    #print(f'starting memory used by tracemalloc in GBs = {tracemem_used_start}')\n",
    "    # memory used during processing.\n",
    "    #print(f'ending memory used in GBs [current, peak] = {tracemem_end}')\n",
    "    # memory used by tracemalloc.\n",
    "    #print(f'ending memory used by tracemalloc in GBs = {tracemem_used_end}')\n",
    "    \n",
    "    # see how long the program took to run.\n",
    "    print(f'\\nstep 1 time elapsed = {round(end_time_step1 - start_time_step1, 3)} seconds ({round((end_time_step1 - start_time_step1) / 60, 3)} minutes)')\n",
    "    print(f'step 2 time elapsed = {round(end_time_step2 - start_time_step2, 3)} seconds ({round((end_time_step2 - start_time_step2) / 60, 3)} minutes)')\n",
    "    print(f'step 3 time elapsed = {round(end_time_step3 - start_time_step3, 3)} seconds ({round((end_time_step3 - start_time_step3) / 60, 3)} minutes)')\n",
    "    print(f'step 4 time elapsed = {round(end_time_step4 - start_time_step4, 3)} seconds ({round((end_time_step4 - start_time_step4) / 60, 3)} minutes)')\n",
    "    print(f'total time elapsed = {round(end_time - start_time, 3)} seconds ({round((end_time - start_time) / 60, 3)} minutes)')\n",
    "    \n",
    "    print('\\nData processing pipeline has completed successfully.\\n' + '-' * 5)\n",
    "    \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: For larger boxes, e.g. 512-cubed and up, processing will take approximately 1 minute or more...\n",
      "-----\n",
      "\n",
      "Step 1: Determining which database files the user-specified box is found in...\n",
      "-------------------------\n",
      "number of database files that the user-specified box is found in:\n",
      "256\n",
      "\n",
      "Successfully completed.\n",
      "-----\n",
      "\n",
      "Step 2: Recursively breaking down the portion of the user-specified box in each database file into voxels...\n",
      "-------------------------\n",
      "sub-box statistics for the database file(s):\n",
      "-\n",
      "minimum number of sub-boxes to read in a database file:\n",
      "1024\n",
      "maximum number of sub-boxes to read in a database file:\n",
      "1024\n",
      "\n",
      "Successfully completed.\n",
      "-----\n",
      "\n",
      "Step 3: Reading the data from all of the database files and storing the values into a matrix...\n",
      "-------------------------\n",
      "\n",
      "Successfully completed.\n",
      "-----\n",
      "\n",
      "Step 4: Writing the output matrix to a hdf5 file...\n",
      "-------------------------\n",
      "\n",
      "Successfully completed.\n",
      "-----\n",
      "\n",
      "step 1 time elapsed = 12.649 seconds (0.211 minutes)\n",
      "step 2 time elapsed = 65.338 seconds (1.089 minutes)\n",
      "step 3 time elapsed = 595.62 seconds (9.927 minutes)\n",
      "step 4 time elapsed = 10.407 seconds (0.173 minutes)\n",
      "total time elapsed = 685.6 seconds (11.427 minutes)\n",
      "\n",
      "Data processing pipeline has completed successfully.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# user-defined parameters for processing data.\n",
    "# size of the model cube that data will be retrieved for.\n",
    "cube_num = 8192\n",
    "# number of dimensions that model data exists in.  default is 3 (i.e. X, Y, and Z dimensions).\n",
    "cube_dimensions = 3\n",
    "# turbulence dataset name, e.g. \"isotropic8192\" or \"isotropic1024fine\".\n",
    "cube_title = 'isotropic8192'\n",
    "# folder name to write the hdf5 output files to.\n",
    "output_folder_name = 'turbulence_hdf5_output'\n",
    "\n",
    "# user specified box rather for which data values will be retrieved for each point inside the box.\n",
    "# the user should specify the 1-based index range. this code will convert to 0-based index ranges for python.\n",
    "x_range = [1, 512]\n",
    "y_range = [1, 512]\n",
    "z_range = [1, 512]\n",
    "\n",
    "# converts the 1-based axes ranges above to 0-based axes ranges.\n",
    "x_range, y_range, z_range = convert_to_0_based_ranges(x_range, y_range, z_range)\n",
    "\n",
    "# variable of interest, currently set to velocity.\n",
    "var = 'vel'\n",
    "# time point. the user should specify the 1-based timepoint. this code will convert to a 0-based timepoint for python.\n",
    "timepoint = 1\n",
    "\n",
    "# converts the 1-based timepoint above to a 0-based timepoint.\n",
    "timepoint = convert_to_0_based_value(timepoint)\n",
    "\n",
    "# process the data.\n",
    "# -----\n",
    "# create the output folder directory if it does not already exist.\n",
    "dir_path = os.path.dirname(os.path.realpath('__file__')) + '/'\n",
    "output_path = dir_path + output_folder_name + '/'\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "\n",
    "# parse the database files, generate the output_data matrix, and write the matrix to an hdf5 file.\n",
    "output_data = process_data(cube_num, cube_dimensions, cube_title, output_path, x_range, y_range, z_range, var, timepoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data value (\"vel\") for datapoint (3, 4, 7):\n",
      "[0.3287481 0.8840852 4.188107 ]\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# retrieve the data value for a datapoint (X, Y, Z).\n",
    "X = 3\n",
    "Y = 4\n",
    "Z = 7\n",
    "\n",
    "data_value = retrieve_data_for_point(X, Y, Z, output_data, x_range, y_range, z_range)\n",
    "\n",
    "print(f'data value (\"{var}\") for datapoint ({X}, {Y}, {Z}):\\n{data_value}\\n' + '-' * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig = plt.figure(figsize = (11, 8.5), dpi = 300)\n",
    "# #ax = fig.add_subplot(11, projection='3d')\n",
    "# x_range_plot = np.arange(x_range[1] + 1)\n",
    "# y_range_plot = np.arange(y_range[1] + 1)\n",
    "\n",
    "# x_plot, y_plot = np.meshgrid(x_range_plot, y_range_plot)\n",
    "# z_plot = output_data[0, :, :, 1]\n",
    "\n",
    "# #z_min = math.floor(-np.abs(z_plot).max())\n",
    "# #z_max = math.ceil(np.abs(z_plot).max())\n",
    "\n",
    "# colormap = 'viridis'\n",
    "\n",
    "# #cf = plt.contourf(x_plot, y_plot, z_plot, cmap = 'rainbow', levels = 500)\n",
    "# cf = plt.pcolormesh(x_plot, y_plot, z_plot, cmap = colormap, shading = 'auto')\n",
    "# cbar = fig.colorbar(cf)\n",
    "\n",
    "# #plt.show()\n",
    "# plt.savefig(output_path + 'turbulence_8192_plane-contour_plot.png')\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0023009711818284618, 0.0030679615757712823, 0.005368932757599744)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # for trials on  http://turbulence.pha.jhu.edu/webquery/query.aspx\n",
    "# # converts the X, Y, and Z points to the domain of [0, 2*pi]\n",
    "# dxyz=2*math.pi/8192\n",
    "# x=X*dxyz\n",
    "# y=Y*dxyz\n",
    "# z=Z*dxyz\n",
    "# # enter these values in UI\n",
    "# x,y,z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare direct access to cutout\n",
    "To get raw data in HDF5 format one can run a job at http://turbulence.idies.jhu.edu/cutout/jobs.\n",
    "Result will be put on scratch. Here an example reading the result of such a job, using the parameters.txt to find the location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# import json\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # ran job x in [1000,1010], y and z in [1,10]\n",
    "# #folder='/home/idies/workspace/Temporary/gerard/scratch/jobs/__turbcutout__/20211012/20211012094603-148997/'\n",
    "# folder = '/home/idies/workspace/Temporary/mschnau1/scratch/jobs/__turbcutout__/20211112/20211112132339-159781/'\n",
    "# p=f'{folder}parameters.txt' \n",
    "# with open(p,'r') as f:\n",
    "#     pars=json.load(f)\n",
    "# pars\n",
    "\n",
    "# f=f'{folder}isotropic8192.h5' \n",
    "# h5=h5py.File(f,'r')\n",
    "\n",
    "# h5['Velocity_0001'].shape\n",
    "\n",
    "# x=h5['xcoor']\n",
    "# y=h5['ycoor']\n",
    "# z=h5['zcoor']\n",
    "# vel=h5['Velocity_0001']\n",
    "\n",
    "# var='vel'\n",
    "# timepoint=pars['ts']-1   # in cutout time starts at 1\n",
    "    \n",
    "# # choose an offset in the retrieved cutout\n",
    "# #dx = 8; dy = 8; dz = 8;\n",
    "\n",
    "# # 1-based index.\n",
    "# x_range_dx = [3, 101]\n",
    "# y_range_dy = [7, 31]\n",
    "# z_range_dz = [8, 21]\n",
    "\n",
    "# for dx in tqdm(range(x_range_dx[0], x_range_dx[1])):\n",
    "#     for dy in range(y_range_dy[0], y_range_dy[1]):\n",
    "#         for dz in range(z_range_dz[0], z_range_dz[1]):\n",
    "#             v1=vel[dz,dy,dx,:]\n",
    "\n",
    "#             # calculate position in the file\n",
    "#             X=pars['xs']+dx-1\n",
    "#             Y=pars['ys']+dy-1  # cutout starts at 1\n",
    "#             Z=pars['zs']+dz-1\n",
    "\n",
    "#             # gets velocity for all points inside the user specified box.\n",
    "#             #iso_data = iso_cube(cube_num = 8192, cube_dimensions = 3, cube_title = 'isotropic8192')\n",
    "\n",
    "#             #v2 = np.asarray(iso_data.get_iso_point_original(X, Y, Z, var, timepoint, verbose = False))\n",
    "\n",
    "#             # data from above code.\n",
    "#             v3 = retrieve_data_for_point(X, Y, Z, output_data, x_range, y_range, z_range)\n",
    "#             #v3 = output_data[5][5][6]\n",
    "\n",
    "#             # hope this would be zeros\n",
    "#             #v1-v2\n",
    "\n",
    "#             #all_zeros_v1v2 = np.all((v1-v2) == 0)\n",
    "#             all_zeros_v1v3 = np.all((v1-v3) == 0)\n",
    "#             #all_zeros_v2v3 = np.all((v2-v3) == 0)\n",
    "\n",
    "#             #if not (all_zeros_v1v2 and all_zeros_v1v3 and all_zeros_v2v3):\n",
    "#             if not all_zeros_v1v3:\n",
    "#                 print(v1)\n",
    "#                 print(v3)\n",
    "#                 raise Exception(f'({X}, {Y}, {Z}) datapoint did not produce the same results between the different methods')\n",
    "                \n",
    "# print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compares with my cutout as well. \n",
    "# # note: my cutout is ordered (X, Y, Z), as opposed to the website cutout (Z, Y, X). the hdf5 format may require (Z, Y, X), so I may have to change my code.\n",
    "# import h5py\n",
    "# import json\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # ran job x in [1000,1010], y and z in [1,10]\n",
    "# #folder='/home/idies/workspace/Temporary/gerard/scratch/jobs/__turbcutout__/20211012/20211012094603-148997/'\n",
    "# folder = '/home/idies/workspace/Temporary/mschnau1/scratch/jobs/__turbcutout__/20211112/20211112150007-159783/'\n",
    "# p=f'{folder}parameters.txt' \n",
    "# with open(p,'r') as f:\n",
    "#     pars=json.load(f)\n",
    "# pars\n",
    "\n",
    "# f=f'{folder}isotropic8192.h5' \n",
    "# h5=h5py.File(f,'r')\n",
    "\n",
    "# h5['Velocity_0001'].shape\n",
    "\n",
    "# x=h5['xcoor']\n",
    "# y=h5['ycoor']\n",
    "# z=h5['zcoor']\n",
    "# vel=h5['Velocity_0001']\n",
    "\n",
    "# # data from the h5 file stored by my algorithm.\n",
    "# folder_my = '/home/idies/workspace/Storage/mschnau1/persistent/turbulence_hdf5_output/'\n",
    "\n",
    "# f_my=f'{folder_my}isotropic8192_vel_t1_z5-513_y3-514_x2-515.h5' \n",
    "# h5_my=h5py.File(f_my,'r')\n",
    "\n",
    "# h5_my['Velocity_0001'].shape\n",
    "\n",
    "# vel_my=h5_my['Velocity_0001']\n",
    "\n",
    "# var='vel'\n",
    "# timepoint=pars['ts']-1   # in cutout time starts at 1\n",
    "    \n",
    "# # choose an offset in the retrieved cutout\n",
    "# #dx = 8; dy = 8; dz = 8;\n",
    "\n",
    "# # 1-based index.\n",
    "# x_range_dx = [500, 512]\n",
    "# y_range_dy = [450, 512]\n",
    "# z_range_dz = [435, 512]\n",
    "\n",
    "# for dx in tqdm(range(x_range_dx[0], x_range_dx[1])):\n",
    "#     for dy in range(y_range_dy[0], y_range_dy[1]):\n",
    "#         for dz in range(z_range_dz[0], z_range_dz[1]):\n",
    "#             v1=vel[dz,dy,dx,:]\n",
    "#             v1_my = vel_my[dz - z_range[0],dy - y_range[0],dx - x_range[0],:]\n",
    "\n",
    "#             # calculate position in the file\n",
    "#             X=pars['xs']+dx-1\n",
    "#             Y=pars['ys']+dy-1  # cutout starts at 1\n",
    "#             Z=pars['zs']+dz-1\n",
    "\n",
    "#             # gets velocity for all points inside the user specified box.\n",
    "#             #iso_data = iso_cube(cube_num = 8192, cube_dimensions = 3, cube_title = 'isotropic8192')\n",
    "\n",
    "#             #v2 = np.asarray(iso_data.get_iso_point_original(X, Y, Z, var, timepoint, verbose = False))\n",
    "\n",
    "#             # data from above code.\n",
    "#             v3 = retrieve_data_for_point(X, Y, Z, output_data, x_range, y_range, z_range)\n",
    "#             #v3 = output_data[5][5][6]\n",
    "\n",
    "#             # hope this would be zeros\n",
    "#             #v1-v2\n",
    "\n",
    "#             #all_zeros_v1v2 = np.all((v1-v2) == 0)\n",
    "#             all_zeros_v1v3 = np.all((v1-v3) == 0)\n",
    "#             all_zeros_v1v1my = np.all((v1-v1_my) == 0)\n",
    "#             #all_zeros_v2v3 = np.all((v2-v3) == 0)\n",
    "\n",
    "#             #if not (all_zeros_v1v2 and all_zeros_v1v3 and all_zeros_v2v3):\n",
    "#             if not (all_zeros_v1v3 and all_zeros_v1v1my):\n",
    "#                 print(v1)\n",
    "#                 print(v1_my)\n",
    "#                 print(v3)\n",
    "#                 raise Exception(f'({X}, {Y}, {Z}) datapoint did not produce the same results between the different methods')\n",
    "                \n",
    "# print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SciServer.CasJobs as cj\n",
    "# sql = \"\"\"\n",
    "# declare @box dbo.Box=dbo.Box::New(0,0,0,8192,8192,8192)\n",
    "# declare @query dbo.Shape=dbo.Shape::Parse('BOX[0,0,0,513,512,512]')\n",
    "# select * from dbo.fcover('M',13,@box,1,@query)\n",
    "# \"\"\"\n",
    "\n",
    "# df = cj.executeQuery(sql, \"simulationDB\")\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (py38)",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
